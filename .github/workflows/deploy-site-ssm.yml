name: Deploy site (SSM + OIDC + S3, fixed bucket, credential handoff)

on:
  push:
    branches: [ "main" ]
    paths:
      - 'app/**'
      - 'nginx/**'
      - '.github/workflows/deploy-site-ssm.yml'
  workflow_dispatch:

concurrency:
  group: deploy-site-ssm
  cancel-in-progress: false

jobs:
  deploy:
    name: Build, upload to S3, deploy via SSM
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    env:
      AWS_REGION: ${{ vars.AWS_REGION || secrets.AWS_REGION || 'us-east-2' }}
      AWS_ROLE_TO_ASSUME: arn:aws:iam::953331331353:role/GitHubActions-ssg-examples-ssh
      ARTIFACT_S3_BUCKET: ssg-examples-artifacts-953331331353-us-east-2
      NAME_TAG: ssg-examples
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Toolchains for optional SSGs
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      - name: Setup Ruby (for Jekyll)
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.2'
          bundler-cache: true
      - name: Setup Hugo (extended)
        uses: peaceiris/actions-hugo@v3
        with:
          hugo-version: '0.128.2'
          extended: true

      - name: Build app + optional SSGs (/jekyll,/hugo,/eleventy,/astro)
        run: |
          set -euo pipefail
          BUILD="__deploy_root"
          rm -rf "$BUILD"; mkdir -p "$BUILD"
          rsync -av --delete app/ "$BUILD"/

          if [ -d app/jekyll ] && { [ -f app/jekyll/_config.yml ] || [ -f app/jekyll/Gemfile ]; }; then
            pushd app/jekyll >/dev/null
            if [ -f Gemfile ]; then
              bundle install --path vendor/bundle
              JEKYLL_ENV=production bundle exec jekyll build -d "$GITHUB_WORKSPACE/$BUILD/jekyll"
            else
              gem install jekyll bundler --no-document
              JEKYLL_ENV=production jekyll build -d "$GITHUB_WORKSPACE/$BUILD/jekyll"
            fi
            popd >/dev/null
          fi

          if [ -d app/hugo ] && ls app/hugo/config.* >/dev/null 2>&1; then
            pushd app/hugo >/dev/null
            hugo --destination "$GITHUB_WORKSPACE/$BUILD/hugo"
            popd >/dev/null
          fi

          if [ -d app/eleventy ] && { grep -q '"@11ty/eleventy"' app/eleventy/package.json 2>/dev/null || ls app/eleventy/.eleventy.* >/dev/null 2>&1; }; then
            pushd app/eleventy >/dev/null
            if [ -f package-lock.json ]; then npm ci; elif [ -f pnpm-lock.yaml ]; then npm i -g pnpm && pnpm i; else npm install; fi
            npx @11ty/eleventy --output "$GITHUB_WORKSPACE/$BUILD/eleventy"
            popd >/dev/null
          fi

          if [ -d app/astro ] && grep -q '"astro"' app/astro/package.json 2>/dev/null; then
            pushd app/astro >/dev/null
            if [ -f package-lock.json ]; then npm ci; elif [ -f pnpm-lock.yaml ]; then npm i -g pnpm && pnpm i; else npm install; fi
            npx astro build --outDir "$GITHUB_WORKSPACE/$BUILD/astro"
            popd >/dev/null
          fi

      - name: Package site
        run: |
          set -euo pipefail
          tar -czf site.tgz -C "__deploy_root" .
          ls -lh site.tgz

      - name: Configure AWS (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Ensure jq
        run: |
          set -euo pipefail
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update -y && sudo apt-get install -y jq
          fi

      - name: Upload tarball to S3 (fixed bucket)
        id: s3
        env:
          BUCKET: ${{ env.ARTIFACT_S3_BUCKET }}
        run: |
          set -Eeuo pipefail
          trap 'echo "ERR(upload) line $LINENO: $BASH_COMMAND" >&2' ERR
          KEY="deploys/${GITHUB_RUN_ID}/site.tgz"
          echo "Uploading to s3://${BUCKET}/${KEY}"
          aws s3 cp site.tgz "s3://${BUCKET}/${KEY}" --only-show-errors
          echo "key=${KEY}" >> "$GITHUB_OUTPUT"
          echo "bucket=${BUCKET}" >> "$GITHUB_OUTPUT"

      - name: Resolve target instance
        id: target
        run: |
          set -euo pipefail
          if [ -n "${{ secrets.EC2_INSTANCE_ID }}" ]; then
            IID="${{ secrets.EC2_INSTANCE_ID }}"
          else
            IID="$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=${{ env.NAME_TAG }}" "Name=instance-state-name,Values=running" \
              --query "Reservations[].Instances[].InstanceId" --output text | head -n1 || true)"
          fi
          [ -n "$IID" ] || { echo "❌ No running instance found (need EC2_INSTANCE_ID or Name=${{ env.NAME_TAG }})" >&2; exit 1; }
          echo "instance_id=$IID" >> "$GITHUB_OUTPUT"

      - name: Deploy via SSM (use runner creds on EC2 to read S3) + diagnostics
        env:
          TARGET: ${{ steps.target.outputs.instance_id }}
          BUCKET: ${{ steps.s3.outputs.bucket }}
          KEY:    ${{ steps.s3.outputs.key }}
          SHA:    ${{ github.sha }}
          AWS_REGION: ${{ env.AWS_REGION }}
        run: |
          set -Eeuo pipefail
          trap 'echo "ERR(ssm) line $LINENO: $BASH_COMMAND" >&2' ERR

          # Capture runner's temporary AWS creds, base64 to avoid quoting issues
          AKID_B64="$(printf '%s' "$AWS_ACCESS_KEY_ID" | base64 | tr -d '\n')"
          ASEC_B64="$(printf '%s' "$AWS_SECRET_ACCESS_KEY" | base64 | tr -d '\n')"
          ATOK_B64="$(printf '%s' "$AWS_SESSION_TOKEN" | base64 | tr -d '\n')"
          AREG_B64="$(printf '%s' "$AWS_REGION" | base64 | tr -d '\n')"

          TEMPLATE="$(cat <<'EOS'
          set -euo pipefail
          # Injected, base64-decoded runner creds (short-lived) to avoid EC2 role 403
          export AWS_ACCESS_KEY_ID="$(printf %s "__AKID_B64__" | base64 -d)"
          export AWS_SECRET_ACCESS_KEY="$(printf %s "__ASEC_B64__" | base64 -d)"
          export AWS_SESSION_TOKEN="$(printf %s "__ATOK_B64__" | base64 -d)"
          export AWS_DEFAULT_REGION="$(printf %s "__AREG_B64__" | base64 -d)"

          ARTIFACT_BUCKET="__BUCKET__"
          ARTIFACT_KEY="__KEY__"
          SHA="__SHA__"

          get_docroot() {
            if command -v nginx >/dev/null 2>&1; then
              ROOTS="$(sudo nginx -T 2>/dev/null | awk "/server_name _;|server_name _ default_server;|server_name _ default;|server_name _;/{flag=1} flag && /root /{print \$2}" | tr -d ';' | head -n1 || true)"
              if [ -n "$ROOTS" ]; then echo "$ROOTS"; return 0; fi
            fi
            echo "/usr/share/nginx/html"
          }
          DOCROOT="$(get_docroot)"

          TMP="$(mktemp -d)"
          PKG="$TMP/site.tgz"
          WORK="$TMP/unpack"
          mkdir -p "$WORK"

          echo "==> Downloading artifact from s3://${ARTIFACT_BUCKET}/${ARTIFACT_KEY}"
          aws s3 cp "s3://${ARTIFACT_BUCKET}/${ARTIFACT_KEY}" "$PKG"

          echo "==> Unpacking"
          tar -xzf "$PKG" -C "$WORK"

          echo "==> Deploying to DOCROOT: $DOCROOT"
          sudo mkdir -p "$DOCROOT"
          sudo rsync -av --delete "$WORK"/ "$DOCROOT"/

          echo "==> Ensure SSG subdirs exist"
          for d in jekyll hugo eleventy astro; do sudo mkdir -p "$DOCROOT/$d"; done

          echo "==> Nginx test & reload (if present)"
          if command -v nginx >/dev/null 2>&1; then
            sudo nginx -t && (sudo systemctl reload nginx || sudo nginx -s reload || true)
          fi

          echo "==> Listing DOCROOT (top)"
          sudo ls -lah "$DOCROOT" | sed -n "1,80p" || true

          echo "===== DIAGNOSTIC SUMMARY ====="
          echo "BUCKET_USED=$ARTIFACT_BUCKET"
          echo "KEY_USED=$ARTIFACT_KEY"
          echo "DOCROOT=$DOCROOT"
          echo "HAS_INDEX=$([ -f "$DOCROOT/index.html" ] && echo 1 || echo 0)"
          echo "=============================="

          rm -rf "$TMP"
          EOS
          )"

          REMOTE_SCRIPT="$TEMPLATE"
          REMOTE_SCRIPT="${REMOTE_SCRIPT//__AKID_B64__/$AKID_B64}"
          REMOTE_SCRIPT="${REMOTE_SCRIPT//__ASEC_B64__/$ASEC_B64}"
          REMOTE_SCRIPT="${REMOTE_SCRIPT//__ATOK_B64__/$ATOK_B64}"
          REMOTE_SCRIPT="${REMOTE_SCRIPT//__AREG_B64__/$AREG_B64}"
          REMOTE_SCRIPT="${REMOTE_SCRIPT//__BUCKET__/$BUCKET}"
          REMOTE_SCRIPT="${REMOTE_SCRIPT//__KEY__/$KEY}"
          REMOTE_SCRIPT="${REMOTE_SCRIPT//__SHA__/$SHA}"

          jq -n --arg cmd "$REMOTE_SCRIPT" '{commands: [$cmd]}' > params.json

          CID="$(aws ssm send-command \
            --instance-ids "$TARGET" \
            --document-name "AWS-RunShellScript" \
            --comment "Deploy ssg-examples ${SHA}" \
            --parameters file://params.json \
            --region "${AWS_REGION}" \
            --query "Command.CommandId" --output text)"
          echo "SSM CommandId: $CID"

          # Wait and print logs
          while true; do
            STATUS="$(aws ssm get-command-invocation --instance-id "$TARGET" --command-id "$CID" --query 'Status' --output text 2>/dev/null || echo 'Unknown')"
            echo "SSM status: $STATUS"
            case "$STATUS" in
              Success) break ;;
              Failed|Cancelled|TimedOut) echo "❌ SSM Run Command failed: $STATUS"; break ;;
              *) sleep 4 ;;
            esac
          done

          OUT_JSON="$(aws ssm get-command-invocation --instance-id "$TARGET" --command-id "$CID" --plugin-name 'aws:runShellScript' --output json 2>/dev/null || echo '{}')"
          echo "----- Remote STDOUT (first 200 lines) -----"
          echo "$OUT_JSON" | jq -r '.StandardOutputContent // ""' | sed -n '1,200p'
          echo "----- Remote STDERR (first 200 lines) -----"
          echo "$OUT_JSON" | jq -r '.StandardErrorContent // ""' | sed -n '1,200p'

          FINAL_STATUS="$(echo "$OUT_JSON" | jq -r '.Status // empty')"
          if [ "$FINAL_STATUS" != "Success" ]; then
            echo "❌ Final SSM plugin status: ${FINAL_STATUS:-unknown}"
            exit 1
          fi

          echo "✅ Remote deploy completed."
